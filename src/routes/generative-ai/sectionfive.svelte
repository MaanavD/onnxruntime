<div class="container">
  <div class="spacer"></div>
  <h2 class="card-title justify-center">Benefits of ONNX Runtime</h2>
  <div class="grid gap-10 grid-cols-1 lg:grid-cols-3 pb-6 pt-6">
    <div class="card bg-base-200">
      <div class="card-body">
        <h2 class="card-title pb-4">Efficiency</h2>
       <p>ONNX Runtime boosts machine learning model performance, enabling real-time generation of text, images, or music in generative AI.</p>
      </div>
    </div>
    <div class="card bg-base-200">
      <div class="card-body">
        <h2 class="card-title pb-4">Flexibility</h2>
      <p>ONNX Runtime supports diverse models from various frameworks, enabling seamless integration of generative AI techniques. This allows deployment of models from TensorFlow or PyTorch without code rewriting.</p>
      </div>
    </div>
    <div class="card bg-base-200">
      <div class="card-body">
        <h2 class="card-title pb-4">Innovation</h2>
        <p>ONNX Runtime reduces training costs for large models, supports on-device training, and is key for deploying AI models across Microsoft products like Windows, Office, Azure Cognitive Services, and Bing, handling over 20 billion inferences daily.</p>
      </div>
    </div>
</div>
  <div class="spacer"></div>
      <div class="grid gap-10 grid-cols-1 lg:grid-cols-2 pb-5 pt-5">
        <div class="card bg-base-200">
          <div class="card-body">
            <h2 class="card-title pb-4">Run Stable Diffusion outside of a Python environment</h2>
            <div class="card-actions">
              <a href="https://onnxruntime.ai/docs/tutorials/csharp/stable-diffusion-csharp.html" class="btn-primary btn">Inference Stable Diffusion →</a>
            </div>
          </div>
        </div>
        <div class="card bg-base-200">
          <div class="card-body">
            <h2 class="card-title pb-4">Speed up inference of Stable Diffusion on NVIDIA and AMD GPUs</h2>
            <div class="card-actions">
              <a href="https://medium.com/microsoftazure/accelerating-stable-diffusion-inference-with-onnx-runtime-203bd7728540" class="btn btn-primary">Accelerate Stable Diffusion →</a>
            </div>
          </div>
        </div>
        <div class="card bg-base-200">
          <div class="card-body">
            <h2 class="card-title pb-4">Accelerate Background removal by 20-550x with WebGPU</h2>
            <div class="card-actions">
              <a href="https://img.ly/blog/browser-background-removal-using-onnx-runtime-webgpu/" class="btn btn-primary">Speed up Background Removal→</a>
            </div>
          </div>
        </div><div class="card bg-base-200">
          <div class="card-body">
            <h2 class="card-title pb-4">Train ML models right in the browser</h2>
            <div class="card-actions">
              <a href="https://opensource.microsoft.com/blog/2024/02/06/on-device-training-training-a-model-in-browser/" class="btn btn-primary"> Train ML Models →</a>
            </div>
          </div>
        </div><div class="card bg-base-200">
          <div class="card-body">
            <h2 class="card-title pb-4">Optimize inferencing with SD Turbo and SDXL Turbo</h2>
            <div class="card-actions">
              <a href="https://huggingface.co/blog/sdxl_ort_inference" class="btn btn-primary">Optimize SD & SDXL Turbo →</a>
            </div>
          </div>
        </div>
  </div>
 </div>
